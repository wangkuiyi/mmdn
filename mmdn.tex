\documentclass{article}

% Remove indentation before paragraphs and use vertical space to
% separate paragraphs.
\setlength{\parindent}{0cm}
\setlength{\parskip}{3mm}

\usepackage[top=1cm,bottom=1.5cm,left=1.5cm,right=1.5cm,paperwidth=5.5in,paperheight=8in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\V}[1]{\mathbf{#1}}

\begin{document}

\title{A DAG of Latent Topic Models for Multimodal Relational Data}
\author{Yi Wang, Guan Wang, Ramanuja Simha}
\maketitle

We argue in this article that we can make full use of multimodal
relational data to address difficulties of high-dimensionality,
sparsity and cold-start.  We propose to build a deep nerual network by
chaining up latent topic/factor models, e.g., latent Dirichlet
allocation or restricted Boltamann machine, learned from relational
data.

An example of three relational data, company--founder,
company--follows and follows--post, is illustrated in
Figure~\ref{fig:relational-data}.  Suppose that we want to predict
companies' performance (measured by value evaluations or finance
reports) using features derived from the data.

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{relational-data}
  \caption{Relational Data}
  \label{fig:relational-data}
\end{figure}

A straightforward approach is that for each company, use its founders
and follows as features.  However, this is well-known of suffering
from data sparseity.  Existing multimodal modeling techniques would
decompose company--founder and company--follower matrices using latent
topic models and use $t_1$ and $t_2$, latent topics learned from these
matrices respectives, as features.  This solves the sparsity problem
somehow, but still suffers from the difficulty of cold-start.

Consider a cold-start problem that we want to predict the performance
of a new company $c$, which is not in the training data illustrated in
Figure~\ref{fig:relational-data}.  Suppose that the two founders,
$f_1$ and $f_2$, of $c$ exist in our training data, so we can use them
as features, or use them to derive latent topics in type $t_1$ as
features.  However, suppose that the two followers, $l_1$ and $l_2$,
of $c$ are not in the training data.  This means that there is no way
to use them or latent topics of type $t_2$ as features.  However, we
noticed that $l_1$ like posts $\V{o}_1$ and that $l_2$ like posts
$\V{o}_2$.  Can we use this to address this cold-start problem?

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.5]{decompose}
  \caption{Decomposition}
  \label{fig:decompose}
\end{figure}

We think we can.  Our idea is to learn a latent topic model from each
matrix in Figure~\ref{fig:relational-data}.  As illustrated in
Figure~\ref{fig:decompose}, learning from the company-founder matrix
produces count matrix $C(company,t_1)$, the number of times that a
latent topic $t_1$ is assigned to a company, and $C(t_1,founder)$, the
number of times that a founder founded a company for a reason (latent
topic).  It is noticable that count matrices can be used with Bayesian
inference to understand new data.  This can help us addressing the
cold-start problem.

Consider the aforementioned cold-start problem.  Given $C(t_3,post)$
as illustrated in Figure~\ref{fig:decompose}, we can infer
$P(t_3|\V{o}_1)$ and $P(t_3|\V{o}_2)$.  This allows us to find some
followers who are in the training data, or some latent topics of type
$t_2$, as features in our prediction.  Denote followers by $f$, we
have
%
\begin{equation}
  \label{eq:1}
  P(t_2|\V{o}_1,\V{o}_2) = \sum_f \sum_{t_3}  P(t_2|f) P(f|t_3) \left[ \sum_j P(t_3|\V{o}_j) \right]
\end{equation}

Since summations like $\sum_{t_3}$ and $\sum_{f}$ can be intuitively
interpreted as activations in neural networks, we can visualize above
equation using a deep neural network, as illustrated in
Figure~\ref{fig:dnn}.

\end{document}
